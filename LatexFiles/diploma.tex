%\documentclass[12pt,a4paper,twoside,draft]{report}
\documentclass[12pt, a4paper]{report}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[main=russian,english]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{epsfig}
\usepackage{array}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{subfigure}
\usepackage{fancyheadings}
\usepackage{ccfonts}
\usepackage{psfrag}
\usepackage{cite}
\usepackage{euscript}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{epstopdf}
\usepackage[rflt]{floatflt}
\usepackage{floatrow}

\sloppy
\pagestyle{plain}

\newcounter{TaskNumber}
\setcounter{TaskNumber}{1}

\renewcommand{\thesection}{\arabic{section}.}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}

% Определяем недостоющие операторы
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\Argmin}{Argmin}

\theoremstyle{definition}
\newtheorem*{Definition}{Определение}
\theoremstyle{plain}
\newtheorem*{Theorem}{Теорема}
\newtheorem*{Lemma}{Лемма}
\theoremstyle{remark}
\newtheorem*{Remark}{Замечание}
\newtheorem*{Example}{Пример}
\newtheorem*{Consequence}{Следствие}
\theoremstyle{remark}
\newtheorem*{Task}{Задание}
\theoremstyle{definition}
\newtheorem*{ProblemStatement}{Постановка задачи}

\addto\captionsenglish{ \renewcommand*\contentsname{Содержание}}
\addto\captionsenglish{\renewcommand{\figurename}{Рис.}}
\addto\captionsrussian{\def\refname{Список используемой литературы}}


\author{Zavgorodniy Igor, 442}
\title{Отчёт}
\date{}

%\includeonly{Title,0-Introduction,Ch1,Ch2,Ch3,Conclusion,Bibliography}


\begin{document}
\begin{titlepage}
  \begin{center}
    \large
   ФЕДЕРАЛЬНОЕ ГОСУДАРСТВЕННОЕ БЮДЖЕТНОЕ ОБРАЗОВАТЕЛЬНОЕ УЧРЕЖДЕНИЕ\\ ВЫСШЕГО ОБРАЗОВАНИЯ\\ "МОСКОВСКИЙ ГОСУДАРСТВЕННЫЙ УНИВЕРСИТЕТ\\ имени М.В.ЛОМОНОСОВА''\\
    ФИЗИЧЕСКИЙ ФАКУЛЬТЕТ\\КАФЕДРА ФИЗИКО-МАТЕМАТИЧЕСКИХ МЕТОДОВ УПРАВЛЕНИЯ

	\underline{\hspace{12cm}}

    \vfill

    {\LARGE \textbf{Выпускная дипломная работа}}
  	\bigskip
  	
    {\textbf{Обучение с подкреплением\\ в задаче поиска пути в лабиринте}}  	
  	\bigskip


\end{center}
\vfill

\begin{flushright}
Выполнил студент IV курса:\\
Завгородний Игорь Викторович\\
\bigskip
Научный руководитель:\\
Галяев А.А.
%$\labda$
\end{flushright}
\vfill

\begin{center}
  Москва\\2019
\end{center}
\end{titlepage}

\newpage
\section{Введение}
Данная работа посвящена применению метода обучения с подкреплением (Reinforcement Learning) в задаче поиска оптимального пути в трёхмерном лабиринте. Построеннаые математические и програмные модели применимы для описания движения агентов в различных физических системах. Например, описание движения беспилотного летательного аппарата (БПЛА), выполняющего задачи в различных слоях атмосферы, описание движения автономного подводного судна, выполнящего исследования на разной глубине, и так далее.

В результате работы был создан и протестирован алгоритм, позволяющий осуществлять оптимальное управление агентом в трёхмерном лабиринте, имитирующим атмосферу. Метод обучения с подкреплением показал эффективность при обучении агента на заданных лабиринтах, где данные не меняются с течением времени, что, безусловно, отличается от реальных процессов.

\newpage
\tableofcontents

\newpage
\section{Теоретическое введение}
\label{sec:W1}
Современные задачи науки и техники требуют применения современных методов, позволяющих быстро и корректно обратывать большие объёмы данных, ежесекундно поступающих с многочисленных датчиков. Более того, с увеличением объёма задач, стоящих перед кибернетическии агентами, усложняется их поведение. Традиционные методы программирования исчерпывают себя, делая решение современных задач неэффективным по затрачиваемому времени и используемой памяти.\\

Данные проблемы призван преодолеть метод машинного обучения (Machine Learning), фундаментальные основы которого были заложены еще в 1940-1950-х годах прошлого века. Однако бурное развитие подобных методов началось лишь в 1990-х годах вместе с ростом вычислительных мощностей компьютеров.
Достоинством данного метода является отсутсвие необходимости создавать детерменированные алогритмы, полностью покрывающие необходимые сценарии поведения агентов. Машинное обучение позволяет создать агентов нового типа, способных обучаться и строить оптимальные алгоритмы при минимальном воздействии человека.\\

Существует две основных концепции машинного обучения: обучение с учителем, в котором агент обучается производить определённые действия на основании предварительно подготовленных выборок, и обучение без учителя, в котором агент самостоятельно формирует стратегию поведения, опираясь на изменения, производимые его действиями. Обучение с подкреплением принадлежит ко второму типу машинного обучения. Агент перебирает все варианты действий и из всех возможных действий выбирает те, которые принесут ему наибольшее итоговое вознаграждение. Перечисленные концепции называются "методом проб и ошибок" и "отсроченным поощрением"\, они лежат в основе обучения с подкреплением.\\

В данной работе для решения задачи поиска пути в лабиринте применяется метод обучения с подкреплением. Как было сказано ранее, одной из особенностей метода является то, что обучение агента происходит благодаря взаимодействию с окружающей средой. Лабиринт - это и есть среда, предназначенная для экспериментального исследования, в которой движется управляемый агент. Задача поиска пути в лабиринте является одной из ключевых задач в робототехнике, решение которой позволяет создавать системы управления движением автономных роботов (дронов). \\

Метод обучения с подкреплением в общем виде можно представить в качестве марковского процесса принятия решений:
\begin{center}
	 $ (S, A, P_a(s, s'), R_a(s, s')),$ где:
\end{center}
\begin{enumerate}
 	\item $S$ - множество возможных состояний среды,
 	\item $A$ - множество возможных действий агента над средой,
 	\item $P_a(s, s') = P(s_{t+1}=s'\,|\,s_t=s,\:a_t=a)$ - вероятность, что состояние $s$ под действием $a$ во время $t$ перейдёт в состояние $s'$ ко времени $t+1$,
 	\item $R_a(s, s') = R(s_{t+1}=s'\,|\,s_t=s,\:a_t=a)$ - вознаграждение, получаемое после перехода в состояние $s'$ из состояния $s$ с вероятностью $P_a(s,s')$.
\end{enumerate}


Поведение агента описывается следующей цепочкой действий:
\begin{center}
состояние $\rightarrow$ действие
$\rightarrow$ поощрение $\rightarrow$ состояние $\rightarrow$ \\
$\rightarrow$ действие $\rightarrow$ поощрение $\rightarrow$ ... 
\end{center}

\begin{figure}[h]
	\center{\includegraphics[scale = 0.25]{SARSA.pdf}}
	\caption{SARSA-модель}
\end{figure}

В англоязычной литературе данный процесс носит название «SARSA» («State-Action-Reward-State-Action-...»).\\

Вводится некоторая политика (англ. \textit{policy}):
\begin{center}
	$\pi \, : \; S \times A \rightarrow [0, 1]$

$\pi(a\,|\,s) = P(a_t=a\,|\,s_t=s)$ - вероятность действия $a$ в состоянии $s$.
\end{center}

Цель агента - выбрать такую оптимальную политику $\pi$, обозначающую вероятность выбора действия $a$ в состоянии $s$, чтобы при следовании ей сумма вознаграждений, получаемых от среды, была максимальна.
Ожидаемая награда в момент времени $t$ определяется как:

$$R_t = E[r_t \,+\, \gamma r_{t+1} \,+\, \gamma^2 r_{t+2}\, +\,\, ...] = E\left[\sum_{k=0}^{\infty}\gamma^kr_{t+k}\right],$$

где $E[\cdot]$ - математическое ожидание, $\gamma\in(0, 1)$ - коэффициент дисконтирования (англ. \textit{discount rate}).

Долгосрочная стратегия агента в общем случае не подразумевает преследование максимальной выгоды на каждом ромежуточном шаге. Непосредственный выбор стратегии может осуществляться множеством способов.
Введем функцию $Q(s,a)$ , которая парам состояние-действие ставит в соотвествие число. Данное число называется ценностью состояния-действия. Также на каждом временном шаге $ t $ агент получает вознаграждение $ r_{t} $:

$$Q^{\pi}(s, a) = E_{\pi}[R_t|\,s_t=s, a_t=a] = E_{\pi}\left[\sum_{k=0}^{\infty}\gamma^kr_{t+k}\,|\,s_t=s, a_t=a\right],$$

где индекс $\pi$ означает выбор действий в соотвествии с некоторой политикой (\textit{policy}).\\

Эта функция характеризует ожидаемую награду, получаемую агентом стартуя из состояния $s, s \in S$ совершая действие $a, a \in A$, и в дальнейшем действуя в соответсвии с определенной политикой $\pi$.

Отсюда мы можем получить рекурсивную формулу для оценки данной функции:\\

\begin{center}
	$Q_{i+1}^{\pi}(s, a) = E_{\pi}\left[r_t + \gamma\sum_{k=0}^{\infty}\gamma^kr_{t+k+1}\,|\,s_t=s, a_t=a\right] = E_{\pi}\left[r_t + \gamma Q_{i}^{\pi}(s_{t+1}=s', a_{t+1}=a')\,|\,s_t=s, a_t=a\right]$
\end{center}

Однако целью агента является - нахождение оптимальной политики $\pi$, на которой достигается максимальная ожидаемая награда. Таким образом, мы должны найти такую $\pi^*$, которая в результате нам дает максимальное значение action-value функции $Q^*(s, a)$ среди всех существующих политик. Формула для оценки оптимального значения action-value функции определяется следующим образом:
$$Q_{i+1}(s, a) = E[r_t + \gamma max_{a'}Q_i(s', a')\,|\,s, a]$$

При $i \rightarrow \infty$ следует, что $Q_{i}(s, a) \rightarrow Q^*(s, a)$. Данный процесс называется \textit{алгоритмом итерации значений} (англ. \textit{value iteration algorithm}).

\section{Постановка и формализация задачи}

\begin{figure}[h]
	{\includegraphics[scale = 0.6]{dronebw.jpg}}
	\caption{Беспилотный аппарат}
\end{figure}

Рассмотрим движение беспилотного аппарата (агента) в атмосфере (испытательной среде, лабиринте). Задачей агента является сбор грузов в различных точках пространтва и их доставка до точек выгрузки с наименьшими затратами топлива.\\

Агент движется в трёхмерном пространстве, каждому слою атмосферы соотвествует одна координата по оси Z, кроме того, происходит движение в плоскости $oXY$. Каждой точке пространства соотвествует определённое значение плотности атмосферы, от которой зависит расход топлива, требуемого для перемещения.\\

Для описания системы используются следующие величины:
\begin{enumerate}
	\item $\vec{r}\ (x(t),y(t),z(t))$ - координата беспилотного аппарата в трёхмерном пространстве: $x\in[0; l_{x}]$, $y\in[0; l_{y}]$, $z\in[0; l_{z}]$;
	\item $\vec{r}_{c}^{i}(x_{c}(t),y_{c}(t),z_{c}(t))$ - координата $i$-го груза: $x_{c}\in[0; l_{x}]$, $y_{c}\in[0; l_{y}]$, $z_{c}\in[0; l_{z}]$, $i \in [1;N_{c}]$;
	\item $\vec{r}_{p}^{i}(x_{p}^{i},y_{p}^{i},z_{p}^{i})$ - координата $i$-го места доставки $x_{p}^{i}\in[0; l_{x}]$, $y_{p}^{i}\in[0; l_{y}]$, $z_{p}^{i}\in[0; l_{z}]$, $i \in [1;N_{c}]$;	
	\item $\rho (\vec{r}, t)$ - плотность атмосферы в точке пространства:  $\rho \in[\rho_{min}; \rho_{max}]$;
	\item $\mu(\rho (\vec{r}, t))$ - количество топлива, требуемого на преодоление данной точки пространства:  $\mu \in[\mu{min}; \mu{max}]$;	
	\item $F$ - статус наличия груза: $F \in \lbrace 0; 1\rbrace$;
	\item $t$ - время: $t \in[0;T]$.
\end{enumerate}

Множество действий, доступных агенту состоит из восьми элементов:
\begin{enumerate}
	\item \textit{south} - увеличение координаты $y$ на 1 (движение на юг);
	\item \textit{north} - уменьшение координаты $y$ на 1 (движение на север);
	\item \textit{east} - увеличение координаты $x$ на 1 (движение на восток);
	\item \textit{west} - уменьшение координаты $x$ на 1 (движение на запад);
	\item \textit{east} - увеличение координаты $z$ на 1 (движение вверх);
	\item \textit{west} - уменьшение координаты $z$ на 1 (движение вниз);
	\item \textit{pickup} - изменение статуса $F$ до 1 (сбор груза);
	\item \textit{dropoff} - изменение статуса $F$ до 0 (сброс груза).	
\end{enumerate}

\subsection{Постановка общей задачи}
Найти оптимальную стратегию $L$, позволяющую агенту доставить все грузы к точкам назначения в среде, где сопростивление воздуха зависит от плотности атмосферы и других параметров:
\begin{equation*}
	\begin{cases}
		L(\vec{r}_{c}^{i}) = \vec{r}_{p}^{i},\ i \in [1;N_{c}]\\
		\vec{r}_{c}^{i} (0) = \vec{r}_{c0}^{i},\ i \in [1;N_{c}]\\
		\vec{r}_{p}^{i} (0) = \vec{r}_{p}^{i},\ i \in [1;N_{c}]\\
		\vec{r}(0) = \vec{r}_{0} \\
		\rho (\vec{r},...)= f^{\rho}(\vec{r},...) \\
		\mu (\rho)= f^{\mu}(\rho).
	\end{cases}	
\end{equation*}
\subsection{Постановка частной задачи}
Найти оптимальную стратегию $L$, позволяющую агенту доставить все грузы к точкам назначения в среде, где сопростивление воздуха зависит от плотности атмосферы:

\begin{equation*}
\begin{cases}
	L(\vec{r}_{c}^{i}) = \vec{r}_{p}^{i},\ i \in [1;N_{c}]\\
	\vec{r}_{c}^{i} (0) = \vec{r}_{c0}^{i},\ i \in [1;N_{c}]\\
	\vec{r}_{p}^{i} (0) = \vec{r}_{p}^{i},\ i \in [1;N_{c}]\\
	\vec{r}(0) = \vec{r}_{0} \\
	\rho (\vec{r})= f^{\rho}(\vec{r}) \\
	\mu (\rho)= f^{\mu}(\rho).
\end{cases}	
\end{equation*}

В работе будет рассмотрено несколько случаев. Простейший из них определяется следующим образом:
\begin{equation*}
\begin{cases}
\rho (\vec{r}) = \rho (z) = f^{\rho}(z) \\
\mu (\rho) = f^{\mu}(\rho) = -\rho (z).
\end{cases}	
\end{equation*}

\section{Применение обучения с подкреплением}

Поставленные задачи необходимо формализовать для применения метода обучения с подкреплением. Агентом является беспилотный аппарат, атмосфера выполняет роль внешней среды, обучающей агента. Взаимодействие со средой происходит при каждом действии агента на протяжении заданного промежутка времени или до достижения терминального состояния. На каждом временном шаге $ t $ агент получает некоторое описание состояния окружающей среды $ s_{t} \in S$, где $ S $ — множество возможных состояний среды, и на основании этого описания выбирает действие, где $ A(s_{t}) $ — множество действий, возможных в состоянии $s_{t}$.\\

Введем функцию $Q(s,a)$ , которая парам состояние-действие соотносит число. Данное число называется ценностью состояния-действия. Также на каждом временном шаге $t$ агент получает вознаграждение $ r_{t} $. В данной задаче вознаграждение (штраф) выдается после каждого действия. При движении через любую клетку на агента накладывается штраф, зависящий, в нашем случае, от плотности атмосферы. Чем выше значение плотности атмосферы, тем выше значение лобового сопротивления агента при движении, тем, соотвестенно, выше штраф за преодоление точки пространства. Также штраф накладывается за сброс груза вне точки сброса и выход за границу поля. И наоборот, агенту даётся вознаграждение за каждый груз, доставленный в правильную точку.\\

Агент придерживается \textit{"жадной" стратегии}, она заключается в жадном выборе действия (действие, которое максимизирует $ Q(s, a) $ ) с вероятностью $ (1 - \epsilon) $ , в остальных случаях действие выбирается случайно. Все используемые в работе методы построения оценки функции ценности пар состояний-действий основаны на методе временных различий (TD — Temporal-Difference). В TD-методах процесс обучения основывается на опыте взаимодействия агента со средой без использования модели среды. Расчетные оценки состояний (в случае задачи управления состояний-действий) в TD-методах обновляются, основываясь на других полученных оценках, т.е. они самонастраиваются [2]. Классический TD-метод используют для построения оценок ценности состояния среды. Опишем его, перед тем как перейти к случаю управления. В данной работе будут использоваться идеи многошагового TD-метода, так же известного как метод TD( $\lambda $), и одношагового метода, или метода TD(0), который является частным случаем многошагового. В многошаговом методе имеется переменная памяти e(s) , соответствующая каждому состоянию. Она называется следом приемлемости [2]. На каждом временном шаге следы приемлемости для всех состояний, кроме текущего, убывают с коэффициентом $ \lambda * \gamma $ , а след приемлемости для посещаемого на данном шаге состояния увеличивается на 1, где $\lambda $ - параметр затухания следа, $\gamma $ - коэффициент приведения. След приемлемости все время регистрирует, посещение каких состояний имело место недавно, где смысл понятия "недавно" определяется с помощью коэффициента $ \lambda *\gamma $. Процесс оценки состояний проходит следующим образом. Во время обучения при переходе из состояний $s_{t}$ в состояние $s_{t+1}$ вычисляется величина $\delta$:
\begin{center}
	$ \delta = r + \gamma V(s_{t+1}) - V(s_{t}),$
\end{center}

где $ V(s_{t}) $ — функция ценности состояния, аналогичная функции ценности пар состояний-действий $ Q(s, a)  $. Далее для всех состояний производится корректировка их ценности с использованием следов приемлемости:
\begin{center}
	$  V(s): = V(s) + \alpha\delta e(s),$
\end{center}
где $\alpha$ — коэффициент обучения. Соответственно, в случае одношагового метода никаких следов приемлемости нет, т.к. $\lambda = 0 $, поэтому на каждом шаге
производится только корректировка ценности состояния $s_{t}$ , что можно записать в виде:
\begin{center}
	$ V(s): = V(s) + \alpha(r + \gamma V(s_{t+1}) - V(s_{t})). $
\end{center}

Одним из наиболее важных достижений в обучении с подкреплением стало развитие управления по TD-методу с разделенной оценкой ценности
стратегий, известного как Q-обучение. В данной работе используется простейший одношаговый алгоритм корректировки ценностей пар состояние-
действие, который основывается на одношаговом методе TD:
\begin{center}
	$ Q(s, a): = Q(s, a) + \alpha(r + \gamma max_{a'} Q(s', a') - Q(s, a)), $
\end{center}
с штрихами здесь состояния и действия $ s_{t+1} $ и $ a_{t+1} $ , без штрихов $ s_{t} $ и $ a_{t} $. В этом случае искомая функция ценности действия Q непосредственно аппроксимирует оптимальную функцию ценности действий, независимо от применяющейся стратегии.
\section{Программная реализация}

Основой для решения послужила библиотека Gym от OpenAI. Библиотека содержала, рассмотренную мной задачу в упрощённом виде: обучение с подкреплением использовалось для оптимизации обработки заказов и движения такси в двумерном лабиринте.\\
Несмотря на кажущуюся схожесть с задачей управления беспилотным аппаратом, требовалась серьёзная доработка существующего решения:

\begin{itemize}
\item Требовалось обощить задачу на случай движения в трёх измерениях;
\item Требовалось изменить постановку задачи так, чтобы добавить физический и прикладной смыслы.
 \end{itemize}

Обе задачи были выполнены.\\ 

Основные компоненты:
\begin{itemize}
\item main.py - основной файл, в котором реализовано обучение агента с помощью Q-learning, заданы параметры обучения ( количетсво эпизодов, максимальное количетсво шагов в эпизоде, параметры Q-learning т.д.)
\item labyrinth.py - файл, в котором реализованы "правила" взаимодействия агента со средой.
\item map\_generation.py - файл, который содержит необходимые функции для построения символьного поля среды, в которой будет происходить обучение агента.
\item discrete.py - вспомогательный файл, который был разработан OpenAI для обучения с подкреплением.
 \end{itemize}

Код каждого файла (будет) представлен в Приложении.
Теперь рассмотрим каждую часть более подробно.
\subsection{Создание среды}

Как отмечалось выше, программа была реализована с помощью библиотеки gym от OpenAI, так же был использован пакет numpy для более удобной работы с матрицами. \\
В моей задаче, среда - это параллелепипед, задаваемый тремя параметрами (длина, ширина, высота). Создание символьного поля производится в файле map\_generation.py (см. Приложение ), в котором c помощью символов +, -, | и : формируется среда, а так же случайным образом расставляются пункты назначения для агента (R(ed), G(green), B(lue), Y(ellow)) ( см. рисунок ниже ).
\begin{figure}[h!]
	\center{\includegraphics[scale = 0.70]{Location.jpg}}
	\caption{Слой в какой-то момент времени.}
\end{figure}

Основная цель данного символьного поля - провизуализировать перемещение агента в среде. Как отмечалось в разделе " Постановка и формализация задачи ", агенту доступны следующие действия (actions):
\begin{itemize}
\item Двигаться на юг (move south)
\item Двигаться на север (move north)
\item Двигаться на восток (move east)
\item Двигаться на запад (move west)
\item Двигаться наверх (move up)
\item Двигаться вниз (move down)
\item Подобрать объект (pickup)
\item Положить объект (dropoff)
 \end{itemize}

За каждое действие, агент получается очки (rewards). Они могут быть как очки вознаграждения, когда агент доставил объект из одной точки в другую, так и очки штрафы, когда агент сделал непраивльное действие, например, доставил объект не в то место или врезался в препятствие. Кроме того, за каждое перемещение агент теряет очки (топливо). И в зависимости от того, в какой ячейке находится агент, он зтрачивает различное количество очков. За то, сколько необходимо потратить на перемещение отвечает функционал, который каждому набору данных в ячейке ставит в соответствие вознаграждение. В самом простом случае, в ячейке хранится уровень слоя, но в моей реализации среда также может учитывать плотность, давление и споротивление воздуха на данной высоте. При проведении различных испытаний, связанных с изменнием количества эпизодов обучения агента, размеров среды и т.д. учитывается только высота слоя, поэтому вознаграждние рассчитывается следюущим образом:
\begin{center}
    reward = lay\_reward(lay),
\end{center}
где lay\_reward - это структура данных "ключ-значение", где ключ - это номер слоя, а значение - очки на этом слое. В моём случае, нулевой слой соответсвует вознаграждению -1, а n-ый слой вознаграждению -n. 

Теперь рассмотрим количетсво возможных состояний в данной задаче. Всю среду можно представить в виде трехмерной сетки size\_x*size\_y*size\_z. Количетсво ячеек этой сетки равно количетсву возможных расположений агента. В среде так же расположены 4 возможных места назначения. Если еще учесть одно состояние объекта: объект находится у агента, то можно подсчитать общее количество состояний в нашей среде для обучения агента. Итого, четыре возможных расположений пунктов назначений и 5 возможных расположений для объекта. Следовательно, в нашей среде насчитывается 
\begin{center}
    N = size\_x*size\_y*size\_z*5*4
\end{center}
возможных состояний для агента. Агент взаимодейтсвует с одним из этих состояний и предпринимает решение, какое действие ему принять дальше.\\

После того, как было задано количество состояний, нужно учесть границы среды, чтобы в дальнейшем агент не смог за них выйти. Основную часть данного файла занимает шестивложенный цикл по следующим парметрам:
\begin{itemize}
    \item 3 пространственных параметра (lay, row, column)
    \item 2 по состояним объекта и пунктов назначения
    \item 1 по возможным действиям 
\end{itemize}
Внутри данного шестивложенного цикла происходит заполнение первичной таблицы вознаграждений под названием P. Данная таблица является матрицей, в котором количество столбцов соотвествует числу возможных действий, а количество строк соотвествует количеству состойний. На рисунке ниже представлена данная матрица P при рандомном индексе 442.
\begin{figure}[h!]
	\center{\includegraphics[scale = 0.70]{Actions_States.jpg}}
	\caption{P[442]}
\end{figure}
\\
Как интерпретировать эти данные?
\begin{center}
    (action, [(probability, nextstate, reward, done)]),
\end{center}
Причем, 
\begin{itemize}
    \item значения 0 - 7 соответсвуют действиям (south, north, east, west, move up, move down, pickup, dropoff)
    \item done характеризует результат доставки объекта в пункт назначения
\end{itemize}
Каким же образом заполняется данная таблица? В шестивложенном цикле существует проверка на то, какое действие совершается и в зависимости от результата дейтсвия агент получает опредленное количество очков. Новое состояние получается при помощи функции encode и пяти параметров:
\begin{itemize}
    \item 3 пространственных (new\_lay, new\_row, new\_col)
    \item расположения объекта (new\_pass\_idx)
    \item расположения пункта назначения (desc\_idx)
\end{itemize}
В данном файле так же представлена функция render(), которая реализуют 2D-отрисовку перемещения агента в среде. В render используется decode(), которая преобразует входные данные в расположение агента, объекта и пунктов назначения при визуалиции. Ниже представлено несколько последовательных расположений агента в среде в виде куба со стороной 5:

Задача для агента формулируется следующим образом: "Доставить объект из пункта А в пункт Б с минимальными затратами топлива."

\subsection{Основной алгоритм}

Прежде всего, нужно заметить, что данную задачу можно решить другим способом без машинного обучения. С помощью цикла while можно написать алгоритм, который реализовывал бы доставку объекта и одной точки локации в другую, но, очевидно, что данный алгоритм был бы совершенно не эффективен на сетках любой размерности. Теперь перейдем к описанию самого алгоритма. \\

Используя среду, которую я описал в предыдущем подпункте и gym, я реализовал Q-learning алгоритм для поставленной задачи. Перед тем, как описывать реализацию алгоритма необходимо описать несколько полезных функций, которые были разработаны OpenAI. Прежде всего, env = gym.make() - это сердце OpenAI Gym, представляет собой интерфейс среды. У env есть несколько полезных методов:
\begin{itemize}
    \item env.step(action) - продвигает развитие оружающей среды на один шаг по времени
    \item env.reset - перезапускает среду, то есть перезапускает исходную среду и возвращает новое случайное исходное состояние
\end{itemize}
Напомню основные детали Q-learning метода. Среда вознаграждает агента за постепенное обучение и за то, что в конкретном состоянии он совершает наиболее оптимальный шаг. В предыдущем подпункте я вводил таблицу P, по которой будет учиться агент. Опираясь на таблицу вознаграждений, он выбирает следующее действие в зависимости от того, насколько оно затратно, а затем обновляет величину, именуемую Q-значением. В результате создается новая таблица ( Q-таблица ), отображаемая на комбинацию (State, Action). Если Q-значения оказываются лучше, то получаются более оптимизированные вознаграждения. Например, если агент с объейктом находится в точке, в которой нужно выложить объект, то Q-значение для "dropoff" оказывается выше, чем для остальных действий. При взаимодейтсвии со средой Q-значение в Q-таблицы обновляется на основе следующей формулы:
\begin{center}
    $    Q(state, action) = Q(state,action) + \alpha*[R(state, action) + \gamma*maxQ(state', action') - Q(state,action)],$
\end{center}
где $\alpha$, $\gamma$ - параметры Q-learning. $\alpha$ - это темп обучения, а $\gamma$ - дисконтирующий множитель. Гамма определяет, какую мы хотим придать важность вознаграждениям, ождиюащим нас в перпективе.\\

Так же, чтобы агент был "любопытным" вводится параметр $\epsilon$, отвечающий за так называемый exploration, то есть за исследование среды.
\subsection{Вспомогательный файл от OpenAI}

\section{Результаты работы}
\section{Вывод}
%\begin{itemize}
%\item Удалось синтезировать ПИД-регулятор, удовлетворяющий заданным условиям;
%\item Дополнительное иссоелование системы позволяет говорить об устойчивости системы, достигнутой благодаря синтезу регулятора.
 %\end{itemize}

\newpage
\section{Список используемой литературы} 
$[1]$ Комаров А. Ю., Метод обучения с подкреплением для архитектуры вероятностных автоматов.\\
$[2]$ Князятов С.А., Малинецкий Г.Г.,
Решение задачи распознавания блефа в игре «верю – не верю» с помощью алгоритмов
обучения с подкреплением // Препринты ИПМ им. М.В.Келдыша. 2018. No 170. 21 с.\\
$[3]$ André Barreto, Will Dabney, Rémi Munos, Jonathan J. Hunt,
Tom Schaul, Hado van Hasselt, David Silver, Successor Features for
Transfer in Reinforcement Learning \\
$[4]$ André Barreto, Will Dabney, Rémi Munos, Jonathan J. Hunt,
Tom Schaul, Hado van Hasselt, David Silver, Successor Features for
Transfer in Reinforcement Learning \\
$[5]$ Romain Laroche, Merwan Barlier, Transfer Reinforcement Learning with Shared Dynamics \\
$[6]$ Саттон Р., Барто Э. Обучение с подкреплением – Бином. Лаборатория знаний,
2012. – 400 с.

\end{document} 
